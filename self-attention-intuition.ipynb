{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1d8f12",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; text-decoration: underline ;\">\n",
    "    Self-Attention from Scratch\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be34c2e",
   "metadata": {},
   "source": [
    "##  Imports and data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8328e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from profiler_utils import record_time_function\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "88b3d420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256, 384])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(74)\n",
    "B,T,C = 32,256,384 \n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc48593",
   "metadata": {},
   "source": [
    "## V1 : naive implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33068e",
   "metadata": {},
   "source": [
    "> ### <u> Intuition </u>\n",
    "> - We are simply **averaging across all the previously generated tokens for each seperate batch**  \n",
    "> and then **predicting the next one**.  \n",
    ">\n",
    "> - While this approach is quite **lossy**, it’s still a **good starting point** —  \n",
    "> because the information that seems lost can still be recovered later.\n",
    "> \n",
    "> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0d91fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@record_time_function(runs=5)\n",
    "def naive_implementation():\n",
    "    xbow = torch.zeros((B,T,C))\n",
    "    for b in range(B):\n",
    "        for t in range(T):\n",
    "            xprev = x[b,:t+1]\n",
    "            xbow[b,t] = torch.mean(xprev,0)\n",
    "    return xbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1a1209af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUDA] naive_implementation (defined in 2484760147.py): 465.6422 ms (avg over 5 runs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[    -0.4402,     -1.2658,     -1.8452,  ...,     -0.8260,\n",
       "              -0.9302,     -0.8619],\n",
       "         [     0.3373,     -0.9448,     -0.3450,  ...,      0.5698,\n",
       "              -0.9247,     -0.2553],\n",
       "         [     0.3851,     -0.0229,     -0.2522,  ...,     -0.2147,\n",
       "              -0.8270,     -0.0406],\n",
       "         ...,\n",
       "         [     0.0849,     -0.0396,     -0.0593,  ...,     -0.0612,\n",
       "               0.0360,     -0.0222],\n",
       "         [     0.0793,     -0.0407,     -0.0584,  ...,     -0.0613,\n",
       "               0.0311,     -0.0216],\n",
       "         [     0.0758,     -0.0367,     -0.0510,  ...,     -0.0634,\n",
       "               0.0361,     -0.0229]],\n",
       "\n",
       "        [[     0.0895,      0.4426,     -0.2971,  ...,      0.0378,\n",
       "              -1.9672,     -0.9947],\n",
       "         [     0.0705,      0.3697,     -0.2142,  ...,     -0.6332,\n",
       "              -0.5950,     -0.0885],\n",
       "         [    -0.2398,      0.4682,     -0.0545,  ...,      0.0834,\n",
       "              -0.4633,     -0.7432],\n",
       "         ...,\n",
       "         [     0.0444,     -0.0610,     -0.0032,  ...,     -0.0102,\n",
       "               0.0374,     -0.0227],\n",
       "         [     0.0451,     -0.0573,     -0.0098,  ...,     -0.0149,\n",
       "               0.0348,     -0.0213],\n",
       "         [     0.0498,     -0.0564,     -0.0102,  ...,     -0.0201,\n",
       "               0.0319,     -0.0238]],\n",
       "\n",
       "        [[     2.9085,     -0.4194,     -0.4325,  ...,     -1.3102,\n",
       "              -0.6043,     -1.2846],\n",
       "         [     2.0123,     -0.3084,      0.0857,  ...,     -0.4986,\n",
       "              -0.9020,     -0.5038],\n",
       "         [     0.9174,     -1.0397,     -0.0560,  ...,      0.5842,\n",
       "              -0.6822,     -0.7818],\n",
       "         ...,\n",
       "         [     0.0621,     -0.0624,     -0.0128,  ...,      0.0899,\n",
       "              -0.0705,     -0.0194],\n",
       "         [     0.0661,     -0.0619,     -0.0062,  ...,      0.0915,\n",
       "              -0.0779,     -0.0262],\n",
       "         [     0.0644,     -0.0579,     -0.0093,  ...,      0.0924,\n",
       "              -0.0739,     -0.0236]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[     0.9134,      0.7307,      0.4803,  ...,      1.5673,\n",
       "               0.6565,      0.3563],\n",
       "         [     1.2969,      0.9764,      0.1996,  ...,      0.5642,\n",
       "              -0.2194,      0.3614],\n",
       "         [     1.1239,      0.7866,      0.1896,  ...,      0.7663,\n",
       "               0.0213,      0.0688],\n",
       "         ...,\n",
       "         [     0.0407,      0.0443,     -0.0140,  ...,     -0.0536,\n",
       "               0.0298,     -0.0633],\n",
       "         [     0.0431,      0.0477,     -0.0150,  ...,     -0.0516,\n",
       "               0.0317,     -0.0574],\n",
       "         [     0.0470,      0.0391,     -0.0106,  ...,     -0.0441,\n",
       "               0.0379,     -0.0566]],\n",
       "\n",
       "        [[    -0.6934,      0.0505,      0.2914,  ...,     -1.5375,\n",
       "               0.1338,     -1.2749],\n",
       "         [    -0.2367,      0.4554,      0.4511,  ...,     -1.5652,\n",
       "              -0.1820,     -0.2244],\n",
       "         [    -0.1174,      0.6625,      0.4364,  ...,     -1.0028,\n",
       "              -0.2937,     -0.8479],\n",
       "         ...,\n",
       "         [     0.0813,     -0.0523,     -0.0602,  ...,      0.0382,\n",
       "               0.0289,      0.0197],\n",
       "         [     0.0810,     -0.0521,     -0.0672,  ...,      0.0392,\n",
       "               0.0268,      0.0199],\n",
       "         [     0.0765,     -0.0547,     -0.0617,  ...,      0.0300,\n",
       "               0.0295,      0.0240]],\n",
       "\n",
       "        [[    -1.0259,      0.7282,     -0.1981,  ...,     -1.1565,\n",
       "               1.2528,     -1.0187],\n",
       "         [    -0.3216,      0.3136,     -0.7915,  ...,     -0.4297,\n",
       "              -0.1681,     -0.2917],\n",
       "         [    -0.4598,      0.4348,     -0.8978,  ...,      0.1317,\n",
       "               0.1361,     -0.4877],\n",
       "         ...,\n",
       "         [     0.0538,     -0.1086,     -0.0036,  ...,     -0.0901,\n",
       "               0.0340,      0.0009],\n",
       "         [     0.0583,     -0.1134,     -0.0038,  ...,     -0.0855,\n",
       "               0.0315,      0.0031],\n",
       "         [     0.0559,     -0.1175,     -0.0014,  ...,     -0.0799,\n",
       "               0.0304,      0.0064]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_implementation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bd723",
   "metadata": {},
   "source": [
    "## V2 : Efficient Averaging using tril and matmul "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b237b0",
   "metadata": {},
   "source": [
    "> ### <u>Intuition</u>\n",
    "> - `tril` gives us the **lower triangular** part of a matrix.  \n",
    "> - When we perform a row-wise normalization `tril` matrix full of ones:\n",
    "> \n",
    "> <div style=\"text-align: center;\">\n",
    "> <pre>\n",
    ">  [1,    0,    0   ]\n",
    ">  [0.5,  0.5,  0   ]\n",
    ">  [0.33, 0.33, 0.33]\n",
    "> </pre>\n",
    "> </div>\n",
    "> \n",
    "> - The **bottom-left triangle** now contains **weights** that sum to 1 in each row,  \n",
    "> while the top-right triangle is zeros.  \n",
    "> \n",
    "> - Now if we perform **matrix multiplication** using `@` <u>(matrix multiplication operator)</u>, we get the same results as if we were performing an average.\n",
    "> \n",
    "> </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8be8870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@record_time_function(runs=100)\n",
    "def efficient_averaging():\n",
    "    wei = torch.tril(torch.ones(T,T))\n",
    "    wei = wei / wei.sum(1, keepdim=True)\n",
    "    xbow2 = wei @ x # wei brodcasts itself 4 times as Batch size is 4 so we multiply each of the \n",
    "    xbow2.shape     # examples in the batch with the normalized matrix\n",
    "    return xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6999dea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CUDA] efficient_averaging (defined in 576775067.py): 4.4733 ms (avg over 100 runs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[    -0.4402,     -1.2658,     -1.8452,  ...,     -0.8260,\n",
       "              -0.9302,     -0.8619],\n",
       "         [     0.3373,     -0.9448,     -0.3450,  ...,      0.5698,\n",
       "              -0.9247,     -0.2553],\n",
       "         [     0.3851,     -0.0229,     -0.2522,  ...,     -0.2147,\n",
       "              -0.8270,     -0.0406],\n",
       "         ...,\n",
       "         [     0.0849,     -0.0396,     -0.0593,  ...,     -0.0612,\n",
       "               0.0360,     -0.0222],\n",
       "         [     0.0793,     -0.0407,     -0.0584,  ...,     -0.0613,\n",
       "               0.0311,     -0.0216],\n",
       "         [     0.0758,     -0.0367,     -0.0510,  ...,     -0.0634,\n",
       "               0.0361,     -0.0229]],\n",
       "\n",
       "        [[     0.0895,      0.4426,     -0.2971,  ...,      0.0378,\n",
       "              -1.9672,     -0.9947],\n",
       "         [     0.0705,      0.3697,     -0.2142,  ...,     -0.6332,\n",
       "              -0.5950,     -0.0885],\n",
       "         [    -0.2398,      0.4682,     -0.0545,  ...,      0.0834,\n",
       "              -0.4633,     -0.7432],\n",
       "         ...,\n",
       "         [     0.0444,     -0.0610,     -0.0032,  ...,     -0.0102,\n",
       "               0.0374,     -0.0227],\n",
       "         [     0.0451,     -0.0573,     -0.0098,  ...,     -0.0149,\n",
       "               0.0348,     -0.0213],\n",
       "         [     0.0498,     -0.0564,     -0.0102,  ...,     -0.0201,\n",
       "               0.0319,     -0.0238]],\n",
       "\n",
       "        [[     2.9085,     -0.4194,     -0.4325,  ...,     -1.3102,\n",
       "              -0.6043,     -1.2846],\n",
       "         [     2.0123,     -0.3084,      0.0857,  ...,     -0.4986,\n",
       "              -0.9020,     -0.5038],\n",
       "         [     0.9174,     -1.0397,     -0.0560,  ...,      0.5842,\n",
       "              -0.6822,     -0.7818],\n",
       "         ...,\n",
       "         [     0.0621,     -0.0624,     -0.0128,  ...,      0.0899,\n",
       "              -0.0705,     -0.0194],\n",
       "         [     0.0661,     -0.0619,     -0.0062,  ...,      0.0915,\n",
       "              -0.0779,     -0.0262],\n",
       "         [     0.0644,     -0.0579,     -0.0093,  ...,      0.0924,\n",
       "              -0.0739,     -0.0236]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[     0.9134,      0.7307,      0.4803,  ...,      1.5673,\n",
       "               0.6565,      0.3563],\n",
       "         [     1.2969,      0.9764,      0.1996,  ...,      0.5642,\n",
       "              -0.2194,      0.3614],\n",
       "         [     1.1239,      0.7866,      0.1896,  ...,      0.7663,\n",
       "               0.0213,      0.0688],\n",
       "         ...,\n",
       "         [     0.0407,      0.0443,     -0.0140,  ...,     -0.0536,\n",
       "               0.0298,     -0.0633],\n",
       "         [     0.0431,      0.0477,     -0.0150,  ...,     -0.0516,\n",
       "               0.0317,     -0.0574],\n",
       "         [     0.0470,      0.0391,     -0.0106,  ...,     -0.0441,\n",
       "               0.0379,     -0.0566]],\n",
       "\n",
       "        [[    -0.6934,      0.0505,      0.2914,  ...,     -1.5375,\n",
       "               0.1338,     -1.2749],\n",
       "         [    -0.2367,      0.4554,      0.4511,  ...,     -1.5652,\n",
       "              -0.1820,     -0.2244],\n",
       "         [    -0.1174,      0.6625,      0.4364,  ...,     -1.0028,\n",
       "              -0.2937,     -0.8479],\n",
       "         ...,\n",
       "         [     0.0813,     -0.0523,     -0.0602,  ...,      0.0382,\n",
       "               0.0289,      0.0197],\n",
       "         [     0.0810,     -0.0521,     -0.0672,  ...,      0.0392,\n",
       "               0.0268,      0.0199],\n",
       "         [     0.0765,     -0.0547,     -0.0617,  ...,      0.0300,\n",
       "               0.0295,      0.0240]],\n",
       "\n",
       "        [[    -1.0259,      0.7282,     -0.1981,  ...,     -1.1565,\n",
       "               1.2528,     -1.0187],\n",
       "         [    -0.3216,      0.3136,     -0.7915,  ...,     -0.4297,\n",
       "              -0.1681,     -0.2917],\n",
       "         [    -0.4598,      0.4348,     -0.8978,  ...,      0.1317,\n",
       "               0.1361,     -0.4877],\n",
       "         ...,\n",
       "         [     0.0538,     -0.1086,     -0.0036,  ...,     -0.0901,\n",
       "               0.0340,      0.0009],\n",
       "         [     0.0583,     -0.1134,     -0.0038,  ...,     -0.0855,\n",
       "               0.0315,      0.0031],\n",
       "         [     0.0559,     -0.1175,     -0.0014,  ...,     -0.0799,\n",
       "               0.0304,      0.0064]]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficient_averaging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd7ed5",
   "metadata": {},
   "source": [
    "## V3 : Adding Softmax to the implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09fdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@record_time_function(runs=100)\n",
    "def softmax_averaging():\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    wei = torch.zeros((T,T))\n",
    "    wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
    "    wei = F.softmax(wei,dim=-1)\n",
    "    xbow3 = wei @ x \n",
    "    return xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c5767cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken by softmax_normalizing: 4.7276 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[    -0.4402,     -1.2658,     -1.8452,  ...,     -0.8260,\n",
       "              -0.9302,     -0.8619],\n",
       "         [     0.3373,     -0.9448,     -0.3450,  ...,      0.5698,\n",
       "              -0.9247,     -0.2553],\n",
       "         [     0.3851,     -0.0229,     -0.2522,  ...,     -0.2147,\n",
       "              -0.8270,     -0.0406],\n",
       "         ...,\n",
       "         [     0.0849,     -0.0396,     -0.0593,  ...,     -0.0612,\n",
       "               0.0360,     -0.0222],\n",
       "         [     0.0793,     -0.0407,     -0.0584,  ...,     -0.0613,\n",
       "               0.0311,     -0.0216],\n",
       "         [     0.0758,     -0.0367,     -0.0510,  ...,     -0.0634,\n",
       "               0.0361,     -0.0229]],\n",
       "\n",
       "        [[     0.0895,      0.4426,     -0.2971,  ...,      0.0378,\n",
       "              -1.9672,     -0.9947],\n",
       "         [     0.0705,      0.3697,     -0.2142,  ...,     -0.6332,\n",
       "              -0.5950,     -0.0885],\n",
       "         [    -0.2398,      0.4682,     -0.0545,  ...,      0.0834,\n",
       "              -0.4633,     -0.7432],\n",
       "         ...,\n",
       "         [     0.0444,     -0.0610,     -0.0032,  ...,     -0.0102,\n",
       "               0.0374,     -0.0227],\n",
       "         [     0.0451,     -0.0573,     -0.0098,  ...,     -0.0149,\n",
       "               0.0348,     -0.0213],\n",
       "         [     0.0498,     -0.0564,     -0.0102,  ...,     -0.0201,\n",
       "               0.0319,     -0.0238]],\n",
       "\n",
       "        [[     2.9085,     -0.4194,     -0.4325,  ...,     -1.3102,\n",
       "              -0.6043,     -1.2846],\n",
       "         [     2.0123,     -0.3084,      0.0857,  ...,     -0.4986,\n",
       "              -0.9020,     -0.5038],\n",
       "         [     0.9174,     -1.0397,     -0.0560,  ...,      0.5842,\n",
       "              -0.6822,     -0.7818],\n",
       "         ...,\n",
       "         [     0.0621,     -0.0624,     -0.0128,  ...,      0.0899,\n",
       "              -0.0705,     -0.0194],\n",
       "         [     0.0661,     -0.0619,     -0.0062,  ...,      0.0915,\n",
       "              -0.0779,     -0.0262],\n",
       "         [     0.0644,     -0.0579,     -0.0093,  ...,      0.0924,\n",
       "              -0.0739,     -0.0236]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[     0.9134,      0.7307,      0.4803,  ...,      1.5673,\n",
       "               0.6565,      0.3563],\n",
       "         [     1.2969,      0.9764,      0.1996,  ...,      0.5642,\n",
       "              -0.2194,      0.3614],\n",
       "         [     1.1239,      0.7866,      0.1896,  ...,      0.7663,\n",
       "               0.0213,      0.0688],\n",
       "         ...,\n",
       "         [     0.0407,      0.0443,     -0.0140,  ...,     -0.0536,\n",
       "               0.0298,     -0.0633],\n",
       "         [     0.0431,      0.0477,     -0.0150,  ...,     -0.0516,\n",
       "               0.0317,     -0.0574],\n",
       "         [     0.0470,      0.0391,     -0.0106,  ...,     -0.0441,\n",
       "               0.0379,     -0.0566]],\n",
       "\n",
       "        [[    -0.6934,      0.0505,      0.2914,  ...,     -1.5375,\n",
       "               0.1338,     -1.2749],\n",
       "         [    -0.2367,      0.4554,      0.4511,  ...,     -1.5652,\n",
       "              -0.1820,     -0.2244],\n",
       "         [    -0.1174,      0.6625,      0.4364,  ...,     -1.0028,\n",
       "              -0.2937,     -0.8479],\n",
       "         ...,\n",
       "         [     0.0813,     -0.0523,     -0.0602,  ...,      0.0382,\n",
       "               0.0289,      0.0197],\n",
       "         [     0.0810,     -0.0521,     -0.0672,  ...,      0.0392,\n",
       "               0.0268,      0.0199],\n",
       "         [     0.0765,     -0.0547,     -0.0617,  ...,      0.0300,\n",
       "               0.0295,      0.0240]],\n",
       "\n",
       "        [[    -1.0259,      0.7282,     -0.1981,  ...,     -1.1565,\n",
       "               1.2528,     -1.0187],\n",
       "         [    -0.3216,      0.3136,     -0.7915,  ...,     -0.4297,\n",
       "              -0.1681,     -0.2917],\n",
       "         [    -0.4598,      0.4348,     -0.8978,  ...,      0.1317,\n",
       "               0.1361,     -0.4877],\n",
       "         ...,\n",
       "         [     0.0538,     -0.1086,     -0.0036,  ...,     -0.0901,\n",
       "               0.0340,      0.0009],\n",
       "         [     0.0583,     -0.1134,     -0.0038,  ...,     -0.0855,\n",
       "               0.0315,      0.0031],\n",
       "         [     0.0559,     -0.1175,     -0.0014,  ...,     -0.0799,\n",
       "               0.0304,      0.0064]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_averaging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ce67e",
   "metadata": {},
   "source": [
    "## V4 : Complete self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b1ed8",
   "metadata": {},
   "source": [
    "> as we can see above wei seems to give equal weightage to all tokens, however in theory this is not really true as certain tokens might find other tokens more or less interesting , therefore we make each vector or token emit a key and query vector , this key gives certain key informations that answers the queries that each word might be asking all other other words, the reason this is called self-attention is because the input to all these vectors is the same 'x'\n",
    "> \n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "90b69dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@record_time_function(runs=100)\n",
    "def self_attn():\n",
    "    head_size = 16 \n",
    "    key  = nn.Linear(C,head_size, bias = False)\n",
    "    query  = nn.Linear(C,head_size, bias = False)\n",
    "    value  = nn.Linear(C,head_size, bias = False)\n",
    "\n",
    "    k = key(x)\n",
    "    q = query(x)\n",
    "    v = value(x) # exists because directly using the tokens to aggregate we get a value each token holds \n",
    "                # and aggregate that instead and also to get the output in the head size dimension\n",
    "                \n",
    "    wei = q @ k.transpose(-2,-1) # dot product essentially\n",
    "\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
    "    wei = F.softmax(wei,dim=-1) # normalizing\n",
    "\n",
    "    v = value(x)\n",
    "    xbow4 = wei @ v\n",
    "    return xbow4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "69ff0fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken by self_attn: 4.9463 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1047, -0.0934,  0.4096,  ..., -0.9040, -0.0155,  0.9516],\n",
       "         [ 0.0608, -0.3139,  0.4058,  ...,  0.2095,  0.0601, -0.1607],\n",
       "         [ 0.7137, -0.0043, -0.2764,  ..., -0.7193, -0.3178, -0.0263],\n",
       "         ...,\n",
       "         [ 0.0494,  0.0107,  0.0033,  ..., -0.0117, -0.1115, -0.1293],\n",
       "         [ 0.1249, -0.1412,  0.1612,  ..., -0.0303, -0.1349, -0.3617],\n",
       "         [-0.0850, -0.1028, -0.2304,  ..., -0.0188,  0.0623, -0.0946]],\n",
       "\n",
       "        [[ 0.1180,  1.0841,  0.4757,  ...,  0.6120,  0.0767, -0.5927],\n",
       "         [-1.1127,  0.6219,  0.3231,  ..., -0.0248, -0.0586,  0.7324],\n",
       "         [-0.2012,  0.4842,  0.4768,  ...,  0.0085,  0.0180,  0.0816],\n",
       "         ...,\n",
       "         [ 0.0130,  0.1304,  0.0526,  ...,  0.0698, -0.0454, -0.1068],\n",
       "         [-0.0121,  0.1925,  0.3428,  ..., -0.1335, -0.0814, -0.3686],\n",
       "         [-0.0101,  0.1342,  0.1216,  ..., -0.0432,  0.0320, -0.0691]],\n",
       "\n",
       "        [[-0.0757,  0.7129,  1.2230,  ...,  0.1801,  0.0882, -1.0942],\n",
       "         [ 0.0562,  0.4837,  0.1702,  ...,  0.5899,  0.2472, -0.3853],\n",
       "         [ 0.1388,  0.1205, -0.2257,  ...,  0.6171,  0.3339, -0.1798],\n",
       "         ...,\n",
       "         [ 0.0498,  0.1068, -0.0056,  ...,  0.0097,  0.0185,  0.0019],\n",
       "         [ 0.0683,  0.1306,  0.0591,  ..., -0.0721, -0.1018, -0.1030],\n",
       "         [ 0.2005, -0.0677, -0.1262,  ...,  0.0618, -0.1021,  0.1132]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5592, -0.1531, -1.6713,  ..., -0.1073,  0.6431,  0.2670],\n",
       "         [ 0.0834,  0.7136, -0.1044,  ..., -0.4275, -0.5943,  0.3896],\n",
       "         [ 0.5429, -0.1375, -1.6315,  ..., -0.1147,  0.6253,  0.2714],\n",
       "         ...,\n",
       "         [ 0.0316,  0.2139, -0.0025,  ...,  0.2667,  0.0223, -0.1007],\n",
       "         [ 0.1182,  0.0908,  0.1342,  ..., -0.0360,  0.0157, -0.0666],\n",
       "         [ 0.2115, -0.0242, -0.0966,  ..., -0.0475,  0.1069, -0.0711]],\n",
       "\n",
       "        [[-0.2534, -0.5954,  0.6599,  ...,  0.4092,  0.2939,  1.2926],\n",
       "         [-0.1121,  0.1493, -0.5446,  ..., -1.0800, -0.0512, -0.4475],\n",
       "         [-0.1847, -0.3034,  0.3660,  ...,  0.0400,  0.1429,  0.8442],\n",
       "         ...,\n",
       "         [-0.0361, -0.0678, -0.0806,  ..., -0.1334, -0.1048,  0.0661],\n",
       "         [ 0.0418, -0.0898, -0.0256,  ...,  0.2307,  0.0763,  0.1590],\n",
       "         [ 0.0277, -0.0183,  0.0018,  ..., -0.0708, -0.0618,  0.1573]],\n",
       "\n",
       "        [[-0.1965, -0.1386,  0.6476,  ..., -0.8475, -0.8688, -0.9532],\n",
       "         [ 0.3852,  0.6978,  0.6481,  ...,  0.4344, -0.1013, -0.4617],\n",
       "         [-0.0239, -0.0697,  0.4193,  ..., -0.4593, -0.6475, -0.7558],\n",
       "         ...,\n",
       "         [ 0.0658,  0.0039, -0.0349,  ...,  0.1816, -0.2386,  0.1100],\n",
       "         [-0.1989, -0.1660,  0.0477,  ...,  0.2074, -0.1436,  0.0926],\n",
       "         [ 0.0146, -0.0508, -0.0303,  ..., -0.0204, -0.0395,  0.0714]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de20464",
   "metadata": {},
   "source": [
    "> ## NOTES\n",
    "> - attention is just a communication mechanism , it can be applied to any arbitrary directed graph\n",
    "> - attention doesnt have a notion of space it operates over sets of vectors therefore it needs positional encoding\n",
    "> - there is no communication across batches, each example in the batch has its own attention\n",
    "> - encoders unlike decoders are not causal, the future can talk to the past, gpt is a decoder only architecture, attention works with both encoders and decoders\n",
    "> - attention can be of many types , cross attention uses the queries from another node and only passes the key and value from current nodes or vice versa\n",
    "> - attention needs to be normalized before softmaxing or it turns into an almost one-hot encoded vector where the major attention is only paid to one token instead of spread across tokens like it should be. high variance before softmaxing leads to almost one-hot like predictions\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb2680",
   "metadata": {},
   "source": [
    "## Why normalizing before softmax matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1)\n",
    "k2 = torch.randn(B,T,head_size)\n",
    "q2 = torch.randn(B,T,head_size)\n",
    "wei2 = q2 @ k2.transpose(-2,-1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce28ac7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0069), tensor(0.9967))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() , k2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77daeb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9302), tensor(0.8952))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() ,  q2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f36ac9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(15.6977), tensor(1.0194))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var() , wei2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb7c6663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax(wei)  [0][0]:\n",
      "tensor([    0.0000,     0.8555,     0.1444,     0.0000,     0.0000,     0.0000,\n",
      "            0.0000,     0.0001])\n",
      "\n",
      "Softmax(wei2) [0][0]:\n",
      "tensor([0.2348, 0.0406, 0.1533, 0.0552, 0.0287, 0.3193, 0.0635, 0.1047])\n"
     ]
    }
   ],
   "source": [
    "print(\"Softmax(wei)  [0][0]:\")\n",
    "print(torch.softmax(wei, dim=-1)[0][0])   # almost one-hot like results\n",
    "\n",
    "print(\"\\nSoftmax(wei2) [0][0]:\")\n",
    "print(torch.softmax(wei2, dim=-1)[0][0])  # better distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-from-scratch (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
