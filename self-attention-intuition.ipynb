{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1d8f12",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; text-decoration: underline ;\">\n",
    "    Self-Attention from Scratch\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be34c2e",
   "metadata": {},
   "source": [
    "##  Imports and data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8328e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b3d420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(74)\n",
    "B,T,C = 4,8,2 \n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc48593",
   "metadata": {},
   "source": [
    "## V1 : naive implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33068e",
   "metadata": {},
   "source": [
    "> ### <u> Intuition </u>\n",
    "> - We are simply **averaging across all the previously generated tokens for each seperate batch**  \n",
    "> and then **predicting the next one**.  \n",
    ">\n",
    "> - While this approach is quite **lossy**, it’s still a **good starting point** —  \n",
    "> because the information that seems lost can still be recovered later.\n",
    "> \n",
    "> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d91fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bd723",
   "metadata": {},
   "source": [
    "## V2 : Efficient Averaging using tril and matmul "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b237b0",
   "metadata": {},
   "source": [
    "> ### <u>Intuition</u>\n",
    "> - `tril` gives us the **lower triangular** part of a matrix.  \n",
    "> - When we perform a row-wise normalization `tril` matrix full of ones:\n",
    "> \n",
    "> <div style=\"text-align: center;\">\n",
    "> <pre>\n",
    ">  [1,    0,    0   ]\n",
    ">  [0.5,  0.5,  0   ]\n",
    ">  [0.33, 0.33, 0.33]\n",
    "> </pre>\n",
    "> </div>\n",
    "> \n",
    "> - The **bottom-left triangle** now contains **weights** that sum to 1 in each row,  \n",
    "> while the top-right triangle is zeros.  \n",
    "> \n",
    "> - Now if we perform **matrix multiplication** using `@` <u>(matrix multiplication operator)</u>, we get the same results as if we were performing an average.\n",
    "> \n",
    "> </br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be8870d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # wei brodcasts itself 4 times as Batch size is 4 so we multiply each of the \n",
    "xbow2.shape     # examples in the batch with the normalized matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd7ed5",
   "metadata": {},
   "source": [
    "## V3 : Adding Softmax to the implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09fdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "xbow3 = wei @ x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5767cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ce67e",
   "metadata": {},
   "source": [
    "## V4 : Complete self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b1ed8",
   "metadata": {},
   "source": [
    "> as we can see above wei seems to give equal weightage to all tokens, however in theory this is not really true as certain tokens might find other tokens more or less interesting , therefore we make each vector or token emit a key and query vector , this key gives certain key informations that answers the queries that each word might be asking all other other words, the reason this is called self-attention is because the input to all these vectors is the same 'x'\n",
    "> \n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b69dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16 \n",
    "key  = nn.Linear(C,head_size, bias = False)\n",
    "query  = nn.Linear(C,head_size, bias = False)\n",
    "value  = nn.Linear(C,head_size, bias = False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x) # exists because directly using the tokens to aggregate we get a value each token holds \n",
    "             # and aggregate that instead and also to get the output in the head size dimension\n",
    "             \n",
    "wei = q @ k.transpose(-2,-1) # dot product essentially\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1) # normalizing\n",
    "\n",
    "v = value(x)\n",
    "xbow4 = wei @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de20464",
   "metadata": {},
   "source": [
    "> ## NOTES\n",
    "> - attention is just a communication mechanism , it can be applied to any arbitrary directed graph\n",
    "> - attention doesnt have a notion of space it operates over sets of vectors therefore it needs positional encoding\n",
    "> - there is no communication across batches, each example in the batch has its own attention\n",
    "> - encoders unlike decoders are not causal, the future can talk to the past, gpt is a decoder only architecture, attention works with both encoders and decoders\n",
    "> - attention can be of many types , cross attention uses the queries from another node and only passes the key and value from current nodes or vice versa\n",
    "> - attention needs to be normalized before softmaxing or it turns into an almost one-hot encoded vector where the major attention is only paid to one token instead of spread across tokens like it should be. high variance before softmaxing leads to almost one-hot like predictions\n",
    ">\n",
    "><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3258ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1) #* head_size**-0.5\n",
    "k2 = torch.randn(B,T,head_size)\n",
    "q2 = torch.randn(B,T,head_size)\n",
    "wei2 = q2 @ k2.transpose(-2,-1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce28ac7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0069), tensor(0.9967))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var() , k2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77daeb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9302), tensor(0.8952))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() ,  q2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f36ac9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(15.6977), tensor(1.0194))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var() , wei2.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb7c6663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax(wei)  [0][0]:\n",
      "tensor([    0.0000,     0.8555,     0.1444,     0.0000,     0.0000,     0.0000,\n",
      "            0.0000,     0.0001])\n",
      "\n",
      "Softmax(wei2) [0][0]:\n",
      "tensor([0.2348, 0.0406, 0.1533, 0.0552, 0.0287, 0.3193, 0.0635, 0.1047])\n"
     ]
    }
   ],
   "source": [
    "print(\"Softmax(wei)  [0][0]:\")\n",
    "print(torch.softmax(wei, dim=-1)[0][0])   # almost one-hot like results\n",
    "\n",
    "print(\"\\nSoftmax(wei2) [0][0]:\")\n",
    "print(torch.softmax(wei2, dim=-1)[0][0])  # better distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-from-scratch (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
